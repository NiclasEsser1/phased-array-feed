# Introduction

This branch 'bmf_bypassing_2' is a special implementation of the phased-array-feed project. Instead of implementing a GPU-back-end pipeline, it bypasses the BMF stage to capture non-beam-formed raw voltage data from the PAF. The GPU-back-end consists of 18 different NUMA nodes, each is able to capture a subgroup of channels (7 MHz in total) and 36 dual-polarized elements. Here the term element is technically speaking a beam processed by the BMF, but by using "special" beam-weights where each weight is set to 0 except a single is set to 1 allows, to forward one dual-polarized element in the PAF per beam. The output of the BMF is limited up to 36 dual-polarized beams on 33 channels, hence we can bypass 36 elements as already mentioned.

BMF bypassing can be divided into two parts, one Controller instance and up to 18 Workers. One worker is running on one NUMA while the controller can be started from any system that has SSH access to the GPU back-end.

# Usage
## Configuration
There a four different files that configure BMF bypassing

-  The number of workers and their distribution on the GPU-back-end depends on `routing_table.csv`. This file has to be uploaded before capturing to Tossix and can be generated by `routing_table.py`.
- In order to configure `psrdada`-ring-buffers a so-called template header file is required. An example can be found in the `config` folder.
- The third file is `config.json` file that contains directory dependencies, ring-buffer configuration and version information.
- The last file is a weight file that needs to be uploaded via the Tossix GUI. In order to generate a bypass weight file consider to use `bypass_bmf.py` from this [repo](https://gitlab.mpcdf.mpg.de/nesser/beamcalculator/-/tree/version_1.1)

* Note: Uploading the routing table can be done manually via Tossix VNC or automatically by `routing_table.py`. The latter caused problems in the past, since it changes the center frequency of the DRX (reason still unkown). The procedure for uploading the routing table as well as the beam weights is described in the Tossix manual.
## Launch the environment
Once the configuration is successfully completed the initial environment can be launch. To do so start the control docker (assumed we are in the root project directory on control system):
`bash Dockerfile/bmf_bypass_controller/run.sh`
If the system couldn't found a valid Kerberos TGT we are asked to initialize one by inserting our LDAP credentials. (The Kerberos authentication is required to access the GPU back-end via SSH)

If the docker is launched we can first make a port scan to verify if all desired nodes/workers are receiving packets on each desired port (specified by `routing_table.csv`)
`python port_scan.py`

## Start capturing
In the last step we will launch all worker containers by executing
`python rollout_capture.py`

What will exactly happen? The script will parse the provided configuration, access all workers via SSH, launches one worker container on each NUMA node, allocates ringbuffers inside the dockers via `dada_db`, starts the disk writer tool `dada_dbdisk` and waits for a signal to start capturing. Caution must be taken when inserted the start signal, since there is no verification whether `dada_db` has completely allocated the memory or not (The roll out is in this way just uni-directional). To make sure that all NUMA nodes are ready for capturing, it is recommended to log in to each node separately and display `htop`. When the memory has been fully allocated, the start signal can be given with ('y'). The `capture_main` process will then executed on each node and  RAM gets written. Since the input data stream is an order of magnitude higher than the `dada_dbdisk` can write to disk, the ring buffers are filled after 25 seconds and `capture_main` gets terminated. Before starting the next capturing or closing the connections to the GPU back-end, make sure `dada_dbdisk` has completely finished work on each NUMA node.

# Contact

Author: Niclas Esser - <nesser@mpifr-bonn.mpg.de>

# ToDo's
- Automatically check the state of each NUMA node - bidirectional communication (Is memory completely allocated? Are data completely written to disk? ... )
- Instead of using SSHConnector the roll out could much safer done by using docker swarm
- Rework `capture_main` (Known problems: First buffer is zero filled, handling packet loss)
